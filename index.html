<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

        <title>3DFAW 2019 - Dense Reconstruction from Video (ICCV 2019)</title>
        <!-- Bootstrap Core CSS -->
        <link href="./bootstrap.min.css" rel="stylesheet">
        <link href="./bootstrap-social.css" rel="stylesheet">
        <link href="./custom.css" rel="stylesheet">
        <!-- Custom CSS -->
        <link href="./modern-business.css" rel="stylesheet">
        <!-- Custom Fonts -->
        <link href="./font-awesome.min.css" rel="stylesheet" type="text/css">
		
		<script src="./jquery.js.download"></script>
		<script src="./bootstrap.min.js.download"></script>

		<style>
			body {
				background-color: #44647d;
				color: #d7dee3;
			}
		</style>		
		
    </head>
    <body>
        <!-- Page Content -->
        <div class="container">
            <!-- Marketing Icons Section -->
            <div class="row" align="center">
				<img src="logo2.jpg" class="img-responsive" align="center">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                <h1 class="page-header first-header text-center" style="border-bottom:none">2<sup>nd</sup> 3D Face Alignment in the Wild Challenge</h1>
				<h2 class="page-header first-header text-center" style="border-bottom:none">- Dense Reconstruction from Video -</h2>
                <h4 class="text-center">In conjunction with <a href="http://iccv2019.thecvf.com">ICCV 2019, Seoul, Korea</a><br>
				Oct 27th- Nov 2nd, 2019</h4>
                <br>
				</div>				
            </div>
	    <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Program</h2>
                </div>

            <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                <table class="table">
                  <tbody>
                    <tr>
                      <th class="col-md-2" scope="row">8:50-9:00</th>
                      <td>Welcome and opening</td>                     
                    </tr>
                    <tr>
                      <th scope="row">9:00-10:00</th>
                      <td>
                        <strong>Invited Talk: Photorealistic Codec Avatar</strong>
                        <p>Shugao Ma (Facebook Reality Labs)</p>
                      </td>
                    </tr>
                    <tr>
                      <th scope="row">10:00-10:20</th>
                      <td>3DFAW Challenge Overview</td>
                    </tr>
                    <tr>
                      <th scope="row">10:20-10:40</th>
                      <td>
		      <strong>Multi-view 3d face reconstruction in the wild using siamese networks</strong>
		      <p>Eduard Ramon Maldonado, Janna Escur, Xavier Giro-i-Nieto</p>
                      </td>
                    </tr>
	            <tr>
                      <th scope="row">10:40-11:00</th>
                      <td>
		      <strong>3D Face Shape Regression From 2D Videos with Multi-reconstruction and Mesh Retrieval</strong>
		      <p>Xiao-Hu Shao, Jiangjing Lyu, Junliang Xing, Lijun Zhang, Xiaobo Li, Xiang-Dong Zhou, Yu Shi</p>
                      </td>
                    </tr>
	            <tr>
                      <th scope="row">11:00-11:20</th>
                      <td>
		      <strong>Face Alignment meets 3D Reconstruction</strong>
		      <p>Yinglin Zheng, Ming Zeng, Xuan Cheng, Hui Li</p>
                      </td>
                    </tr>			  
                    <tr>
                      <th scope="row">11:20-11:30</th>
                      <td>Closing remarks</td>
                    </tr> 
                  </tbody>
                </table>
            </div>
            </div>
	    <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Keynote Speaker</h2>

                    <div class="col-lg-3 col-md-3 col-sm-3">
                        <img src="ShugaoMa_800px.jpg" class="img-responsive img-rounded center-block">    
                        <p class="lead text-center"><a href="https://www.linkedin.com/in/shugaoma">Shugao Ma</a></p>    
                    </div>
            </div>
		
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Description</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        Face alignment - the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints - is critical to face analysis applications, such as identification, facial expression analysis, robot-human interaction, affective computing, and multimedia.
                    </p>
                    <p class="text-justify">
                        3D face alignment approaches have strong advantages over 2D with respect to representational power and robustness to illumination and pose. 3D approaches accommodate a wide range of views. Depending on the 3D model, they easily can accommodate a full range of head rotation. Disadvantages are the need for 3D images and controlled illumination, as well as the need for special sensors or synchronized cameras in data acquisition.
                    </p>
                    <p class="text-justify">
                        Because these requirements often are difficult to meet, 3D alignment from 2D video or images has been proposed as a potential solution. Over the past few years a number of research groups have made rapid advances in dense 3D alignment from 2D video and obtained impressive results. How these various methods compare is relatively unknown. Previous benchmarks addressed sparse 3D alignment and single image 3D reconstruction. No commonly accepted evaluation protocol exists for dense 3D face reconstruction from video with which to compare them.
                    </p>

                    <p class="text-justify">
                        To enable comparisons among alternative methods, we present the 2<sup>nd</sup> 3D Face Alignment in the Wild - Dense Reconstruction from Video Challenge. This topic is germane to both computer vision and multimedia communities. For computer vision, it is an exciting approach to longstanding limitations of single-image 3D reconstruction approaches. For multimedia, 3D alignment would enable more powerful applications. 
                    </p>
                </div>
            </div>
			
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Main track</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        3DFAW is intended to bring together computer vision and multimedia researchers whose work is related to 2D or 3D face alignment. We are soliciting original contributions which address a wide range of theoretical and application issues of 3D face alignment for computer vision applications and multimedia including, including but not limited to:
                        </p><ul>
                            <li>3D and 2D face alignment from 2D dimensional images</li>
                            <li>Model- and stereo-based 3D face reconstruction</li>
                            <li>Dense and sparse face tracking from 2D and 3D dimensional inputs</li>
                            <li>Applications of face alignment</li>
                            <li>Face alignment for embedded and mobile devices</li>
                            <li>Facial expression retargeting (avatar animation)</li>
                            <li>Face alignment-based user interfaces</li>
                        </ul>
                    <p></p>
                </div>
            </div>
			
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Challenge track</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        The 2<sup>nd</sup> 3DFAW Challenge evaluates 3D face reconstruction methods on a new large corpora of profile-to-profile face videos annotated with corresponding high-resolution 3D ground truth meshes. The corpora includes profile-to-profile videos obtained under a range of conditions:
                    </p>
                    <ul>
                        <li>high-definition in-the-lab video, </li>
                        <li>unconstrained video from an iPhone device</li>
                    </ul> 
                    <br>
                    <div class="row">
                        <img src="./data500.jpg" alt="Data collection" class="img-responsive center-block">
                        <p class="text-center"><b>Figure 1.</b> Ground truth mesh and profile-to-profile video of a subject.</p>
                    </div>
                    <p class="text-justify">
                        For each subject, high-resolution 3D ground truth scans were obtained using a Di4D imaging system. The goal of the challenge is to reconstruct the 3D structure of the face from the two different video sources. 
                    </p>
                    
                    	<!--<b>If you are interested in getting 3DFAW Challenge training data please <a href="mailto:sergey.tulyakov@unitn.it?subject=3DFAW Challenge data request&amp;body=I am interested in receiving 3DFAW training data. Could you please provide it to me, when the data is ready?">email us</a> </b>-->
                    <p class="lead">
			<b>If you are interested in getting 3DFAW-Video Challenge training data please <a href="./3DFAW-Video_EULA-ICCV2019.pdf">download and sign the EULA</a> and <a href="mailto:hyang51@binghamton.edu?subject=3DFAW-Video Challenge data request&amp">email the scanned copy back to hyang51(at)binghamton(dot)edu.</a></b>
                    </p>

                </div>
            </div>
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Evaluation and Submissions</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        Please visit the <a href="https://codalab.lri.fr/competitions/572">Codalab page</a> where the competition is hosted. Additionally, the evaluation code can be downloaded <a href="https://github.com/RohithPillai/3dfaw-Evaluation">here</a> for local use by participants. The requirements file for the local environment setup can be downloaded from the same repository. Please report any bugs in the evaluation code in the issues of the repository.
			</p> 
			<p class="text-justify">
			Please note that along with your submission to Codalab competition page, in <b>order to be included in the leaderboard, you are required to submit a short paper containing the description of your method</b>. For paper submission to the workshop, visit our <a href="https://cmt3.research.microsoft.com/3DFAW2019">CMT page</a>. When submitting your paper to CMT, please also email <a href="mailto:rohithkp@andrew.cmu.edu?subject=3DFAW-Video Challenge Codalab username/paper title&amp">rohithkp(at)andrew(dot)cmu(dot)edu</a> with your username/Team name on Codalab, and your workshop paper title, to make it easier to link your paper to the Codalab submissions. 
			</p>
			<p class="text-justify">
			Challenge paper submissions must be written in English and must be sent in PDF format. Each submitted paper must be no longer than four (4) pages, excluding references. Please refer to the ICCV submission guidelines for instructions regarding formatting, templates, and policies. The submissions will be reviewed by the program committee and selected papers will be published in ICCV Workshop proceedings, IEEE Xplore & CVF Open Access.
		    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Dates</h2>
			<p class="lead">
			<b>Please note that the challenge end date has been extended till the 20th of August, and submissions can be made on the Codalab competition till the 20th. Also, please note the extended workshop paper submission deadlines.</b>
		                    </p>
					<p><b>Challenge Track</b>
						<ul>
                            <li>June 27th: Challenge site opens, training data available</li>
                            <li>August 1st: Testing phase begins</li>
                            <li>August 20th: Competition ends - Extended Date (challenge paper submission - optional)</li>
                        </ul>
						</p>
					<p><b>Workshop Track</b>
						<ul>
                            <li>August 31st: Paper submission deadline</li>
                            <li>September 11th: Notification of acceptance</li>
                            <li>September 18th: Camera ready submission</li>
                        </ul>
						</p>

                </div>
            </div>


            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Workshop chairs</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <ul>
                        <li><a href="http://www.laszlojeni.com/">Laszlo A. Jeni</a>, Carnegie Mellon University, USA</li>
                        <li><a href="http://www.pitt.edu/~jeffcohn/">Jeffrey F. Cohn</a>, University of Pittsburgh, USA</li>
                        <li><a href="http://www.cs.binghamton.edu/~lijun/">Lijun Yin</a>, Binghamton University, USA</li>
                    </ul>
              
			<p><b>Data chairs</b>
			<ul>
                        <li><a href="">Rohith Krishnan Pillai</a>, Carnegie Mellon University, USA</li>
                        <li><a href="">Huiyuan Yang</a>, Binghamton University, USA</li>
			<li><a href="">Zheng Zhang</a>, Binghamton University, USA</li>
                        </ul>
			</p>
           
            </div>

            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Technical program committee</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
			<ul>
				<li>Abhinav Dhall, Australian National University, Australia </li>
				<li>Gábor Szirtes, KÜRT Akadémia / bsi.ai</li>
				<li>Hamdi Dibeklioglu, Bilkent University, Turkey</li>
				<li>Michel Valstar, University of Nottingham, UK </li>
				<li>Patrik Huber, University of Surrey, UK </li>
				<li>Sergio Escalera, University of Barcelona, Spain </li>
				<li>Shaun Canavan, University of South Florida, USA </li>
				<li>Vitomir Štruc, University of Ljubljana, Slovenia</li>
				<li>Xiaoming Liu, Michigan State University, USA</li>
				<li>Xing Zhang, A9</li>				
				<li>Zoltan Kato, University of Szeged, Hungary </li>				
				
			</ul>
                </div>
            </div>
		    </div>
		
    
	

	

</div></body></html>
